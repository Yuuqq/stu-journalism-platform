"""
RAG (Retrieval-Augmented Generation) å¼•æ“
åŸºäº TF-IDF çš„æœ¬åœ°çŸ¥è¯†åº“æ£€ç´¢ç³»ç»Ÿ
"""
import os
import re
from pathlib import Path
from typing import List, Dict, Optional
from dataclasses import dataclass

import PyPDF2
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

from .config import get_config


@dataclass
class SearchResult:
    """æ£€ç´¢ç»“æœ"""
    source: str
    content: str
    score: float

    def to_dict(self) -> Dict:
        return {
            "source": self.source,
            "content": self.content,
            "score": self.score
        }


class RAGEngine:
    """
    æœ¬åœ° RAG å¼•æ“

    ç‰¹æ€§:
    - æ”¯æŒ PDFã€TXTã€MD æ–‡ä»¶
    - ä½¿ç”¨å­—ç¬¦çº§ N-gram é€‚é…ä¸­æ–‡
    - æ»‘åŠ¨çª—å£åˆ†å—æé«˜åŒ¹é…ç²¾åº¦
    """

    def __init__(self, corpus_path: Optional[Path] = None):
        self.config = get_config()
        self.corpus_path = corpus_path or self.config.paths.corpus

        self.documents: List[str] = []
        self.filenames: List[str] = []
        self.chunk_metadata: List[Dict] = []  # å­˜å‚¨æ¯ä¸ªå—çš„å…ƒæ•°æ®

        # ä½¿ç”¨å­—ç¬¦çº§ N-gram åŒ¹é…ï¼Œå®Œç¾é€‚é…ä¸­æ–‡
        rag_config = self.config.rag
        self.vectorizer = TfidfVectorizer(
            analyzer='char_wb',
            ngram_range=rag_config.ngram_range
        )
        self.tfidf_matrix = None

        self._index_corpus()

    def _index_corpus(self) -> None:
        """ç´¢å¼•è¯­æ–™åº“"""
        if not self.corpus_path.exists():
            print(f"Warning: Corpus path does not exist: {self.corpus_path}")
            return

        for file_path in self.corpus_path.rglob("*"):
            if file_path.is_file():
                self._process_file(file_path)

        if self.documents:
            self.tfidf_matrix = self.vectorizer.fit_transform(self.documents)
            print(f"RAG Engine indexed {len(self.documents)} chunks from {len(set(self.filenames))} files")

    def _process_file(self, file_path: Path) -> None:
        """å¤„ç†å•ä¸ªæ–‡ä»¶"""
        try:
            text = self._extract_text(file_path)
            if text:
                self._chunk_text(text, file_path.name)
        except Exception as e:
            print(f"Error processing {file_path}: {e}")

    def _extract_text(self, file_path: Path) -> str:
        """ä»æ–‡ä»¶æå–æ–‡æœ¬"""
        suffix = file_path.suffix.lower()

        if suffix == '.pdf':
            return self._extract_pdf(file_path)
        elif suffix in ('.txt', '.md'):
            return self._extract_text_file(file_path)

        return ""

    def _extract_pdf(self, file_path: Path) -> str:
        """æå– PDF æ–‡æœ¬"""
        text_parts = []
        with open(file_path, 'rb') as f:
            reader = PyPDF2.PdfReader(f)
            for page in reader.pages:
                page_text = page.extract_text()
                if page_text:
                    text_parts.append(page_text)
        return "\n".join(text_parts)

    def _extract_text_file(self, file_path: Path) -> str:
        """æå–æ–‡æœ¬æ–‡ä»¶å†…å®¹"""
        with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
            return f.read()

    def _chunk_text(self, text: str, filename: str) -> None:
        """å°†æ–‡æœ¬åˆ†å—"""
        rag_config = self.config.rag
        chunk_size = rag_config.chunk_size
        overlap = rag_config.chunk_overlap
        min_length = rag_config.min_chunk_length

        for i in range(0, len(text), chunk_size - overlap):
            chunk = text[i:i + chunk_size]
            if len(chunk) > min_length:
                self.documents.append(chunk)
                self.filenames.append(filename)
                self.chunk_metadata.append({
                    "start": i,
                    "end": i + len(chunk),
                    "filename": filename
                })

    def query(self, user_query: str, top_k: Optional[int] = None) -> List[SearchResult]:
        """
        æ£€ç´¢ç›¸å…³æ–‡æ¡£

        Args:
            user_query: ç”¨æˆ·æŸ¥è¯¢
            top_k: è¿”å›ç»“æœæ•°é‡

        Returns:
            æ£€ç´¢ç»“æœåˆ—è¡¨ï¼ŒæŒ‰ç›¸å…³åº¦é™åº
        """
        if self.tfidf_matrix is None or not self.documents:
            return []

        top_k = top_k or self.config.rag.top_k

        # æ¸…æ´—æŸ¥è¯¢ï¼ˆç§»é™¤å¸¸è§åœç”¨è¯ï¼‰
        cleaned_query = self._clean_query(user_query)

        # å‘é‡åŒ–å¹¶è®¡ç®—ç›¸ä¼¼åº¦
        query_vec = self.vectorizer.transform([cleaned_query])
        similarities = cosine_similarity(query_vec, self.tfidf_matrix).flatten()

        # è·å– top-k ç»“æœ
        top_indices = similarities.argsort()[:-top_k-1:-1]

        results = []
        threshold = self.config.rag.similarity_threshold

        for idx in top_indices:
            if similarities[idx] > threshold:
                results.append(SearchResult(
                    source=self.filenames[idx],
                    content=self.documents[idx].strip(),
                    score=float(similarities[idx])
                ))

        return results

    def _clean_query(self, query: str) -> str:
        """æ¸…æ´—ç”¨æˆ·æŸ¥è¯¢"""
        # ç§»é™¤å¸¸è§ä¸­æ–‡åœç”¨è¯å’Œæ ‡ç‚¹
        stopwords = r'[ä»€ä¹ˆæ˜¯çš„ï¼Ÿ?å—å¦‚ä½•æ€ä¹ˆä¸ºä»€ä¹ˆ\s]'
        cleaned = re.sub(stopwords, '', query)
        return cleaned if cleaned else query

    def generate_response(self, user_query: str) -> str:
        """
        ç”Ÿæˆ AI å›å¤

        Args:
            user_query: ç”¨æˆ·é—®é¢˜

        Returns:
            æ ¼å¼åŒ–çš„ AI å›å¤
        """
        results = self.query(user_query)

        if not results:
            return self._generate_not_found_response(user_query)

        return self._format_response(user_query, results)

    def _generate_not_found_response(self, query: str) -> str:
        """ç”Ÿæˆæœªæ‰¾åˆ°ç»“æœçš„å›å¤"""
        return (
            f"ğŸ¤– **AI åˆ†æ**ï¼š\n\n"
            f"åœ¨ç°æœ‰è¯¾ç¨‹èµ„æ–™ä¸­æš‚æœªæ‰¾åˆ°å…³äºã€Œ{query}ã€çš„å…·ä½“æè¿°ã€‚\n\n"
            f"**å»ºè®®**ï¼š\n"
            f"- å°è¯•ä½¿ç”¨æ›´å…·ä½“çš„å…³é”®è¯\n"
            f"- æ£€æŸ¥ assets/corpus ç›®å½•ä¸‹æ˜¯å¦å·²æ·»åŠ ç›¸å…³æ–‡æ¡£"
        )

    def _format_response(self, query: str, results: List[SearchResult]) -> str:
        """æ ¼å¼åŒ–æ£€ç´¢ç»“æœä¸ºå›å¤"""
        response = f"ğŸ¤– **åŸºäºæ ¡å†…è¯¾ç¨‹èµ„æ–™çš„ AI å›å¤**ï¼š\n\n"
        response += f"å…³äºã€Œ**{query}**ã€ï¼Œæˆ‘åœ¨èµ„æ–™åº“ä¸­æ‰¾åˆ°äº†ç›¸å…³å†…å®¹ï¼š\n\n"

        for i, res in enumerate(results, 1):
            clean_content = res.content.replace('\n', ' ')
            response += f"> **ğŸ“‘ æ¥æºï¼š{res.source}** (åŒ¹é…åº¦: {res.score:.2f})\n"
            response += f"> *\"...{clean_content}...\"*\n\n"

        return response

    def get_stats(self) -> Dict:
        """è·å–å¼•æ“ç»Ÿè®¡ä¿¡æ¯"""
        return {
            "total_chunks": len(self.documents),
            "total_files": len(set(self.filenames)),
            "files": list(set(self.filenames)),
            "indexed": self.tfidf_matrix is not None
        }


# å…¨å±€ RAG å¼•æ“å®ä¾‹
_rag_instance: Optional[RAGEngine] = None

def get_rag_engine() -> RAGEngine:
    """è·å–å…¨å±€ RAG å¼•æ“å®ä¾‹"""
    global _rag_instance
    if _rag_instance is None:
        _rag_instance = RAGEngine()
    return _rag_instance
