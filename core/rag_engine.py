"""
RAG (Retrieval-Augmented Generation) å¼•æ“
åŸºäº TF-IDF çš„æœ¬åœ°çŸ¥è¯†åº“æ£€ç´¢ç³»ç»Ÿ

è¯¥æ¨¡å—æä¾›ï¼š
- SearchResult: æ£€ç´¢ç»“æœæ•°æ®ç»“æ„
- RAGEngine: æœ¬åœ° RAG å¼•æ“ï¼Œæ”¯æŒ PDFã€TXTã€MD æ–‡ä»¶
- get_rag_engine: è·å–å…¨å±€ RAG å¼•æ“å®ä¾‹

Features:
- ä½¿ç”¨å­—ç¬¦çº§ N-gram é€‚é…ä¸­æ–‡
- æ»‘åŠ¨çª—å£åˆ†å—æé«˜åŒ¹é…ç²¾åº¦
- æ”¯æŒå¢é‡ç´¢å¼•
"""
from __future__ import annotations

import logging
import re
from pathlib import Path
from typing import List, Dict, Optional, Set, FrozenSet

import numpy as np
from numpy.typing import NDArray
from dataclasses import dataclass, field

try:
    import PyPDF2
except ImportError:
    PyPDF2 = None

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

from .config import get_config

# é…ç½®æ—¥å¿—
logger = logging.getLogger(__name__)


@dataclass
class SearchResult:
    """æ£€ç´¢ç»“æœ

    Attributes:
        source: æ¥æºæ–‡ä»¶å
        content: åŒ¹é…çš„æ–‡æœ¬å†…å®¹
        score: ç›¸ä¼¼åº¦åˆ†æ•° (0-1)
    """
    source: str
    content: str
    score: float

    def to_dict(self) -> Dict:
        """è½¬æ¢ä¸ºå­—å…¸"""
        return {
            "source": self.source,
            "content": self.content,
            "score": round(self.score, 4)
        }


class RAGEngine:
    """
    æœ¬åœ° RAG å¼•æ“

    ç‰¹æ€§:
    - æ”¯æŒ PDFã€TXTã€MD æ–‡ä»¶
    - ä½¿ç”¨å­—ç¬¦çº§ N-gram é€‚é…ä¸­æ–‡
    - æ»‘åŠ¨çª—å£åˆ†å—æé«˜åŒ¹é…ç²¾åº¦
    - å»¶è¿Ÿç´¢å¼•ï¼ŒæŒ‰éœ€åŠ è½½
    """

    # æ”¯æŒçš„æ–‡ä»¶æ‰©å±•å
    SUPPORTED_EXTENSIONS: Set[str] = {'.pdf', '.txt', '.md'}

    def __init__(self, corpus_path: Optional[Path] = None):
        """åˆå§‹åŒ– RAG å¼•æ“

        Args:
            corpus_path: è¯­æ–™åº“è·¯å¾„ï¼Œé»˜è®¤ä½¿ç”¨é…ç½®ä¸­çš„è·¯å¾„
        """
        self.config = get_config()
        self.corpus_path = corpus_path or self.config.paths.corpus

        self.documents: List[str] = []
        self.filenames: List[str] = []
        self.chunk_metadata: List[Dict] = []
        self._indexed_files: Set[str] = set()  # å·²ç´¢å¼•çš„æ–‡ä»¶

        # ä½¿ç”¨å­—ç¬¦çº§ N-gram åŒ¹é…ï¼Œå®Œç¾é€‚é…ä¸­æ–‡
        rag_config = self.config.rag
        self.vectorizer = TfidfVectorizer(
            analyzer='char_wb',
            ngram_range=rag_config.ngram_range
        )
        self.tfidf_matrix = None

        self._index_corpus()

    def _index_corpus(self) -> None:
        """ç´¢å¼•è¯­æ–™åº“"""
        if not self.corpus_path.exists():
            logger.warning(f"Corpus path does not exist: {self.corpus_path}")
            return

        file_count = 0
        for file_path in self.corpus_path.rglob("*"):
            if file_path.is_file() and file_path.suffix.lower() in self.SUPPORTED_EXTENSIONS:
                if str(file_path) not in self._indexed_files:
                    self._process_file(file_path)
                    self._indexed_files.add(str(file_path))
                    file_count += 1

        if self.documents:
            self.tfidf_matrix = self.vectorizer.fit_transform(self.documents)
            logger.info(f"RAG Engine indexed {len(self.documents)} chunks from {len(self._indexed_files)} files")

    def _process_file(self, file_path: Path) -> None:
        """å¤„ç†å•ä¸ªæ–‡ä»¶

        Args:
            file_path: æ–‡ä»¶è·¯å¾„
        """
        try:
            text = self._extract_text(file_path)
            if text:
                self._chunk_text(text, file_path.name)
                logger.debug(f"Processed: {file_path.name}")
        except Exception as e:
            logger.error(f"Error processing {file_path}: {e}")

    def _extract_text(self, file_path: Path) -> str:
        """ä»æ–‡ä»¶æå–æ–‡æœ¬"""
        suffix = file_path.suffix.lower()

        if suffix == '.pdf':
            return self._extract_pdf(file_path)
        elif suffix in ('.txt', '.md'):
            return self._extract_text_file(file_path)

        return ""

    def _extract_pdf(self, file_path: Path) -> str:
        """æå– PDF æ–‡æœ¬"""
        if PyPDF2 is None:
            logger.warning("PyPDF2 not installed, skipping PDF file")
            return ""

        text_parts: List[str] = []
        try:
            with open(file_path, 'rb') as f:
                reader = PyPDF2.PdfReader(f)
                for page in reader.pages:
                    page_text = page.extract_text()
                    if page_text:
                        text_parts.append(page_text)
        except Exception as e:
            logger.error(f"Error reading PDF {file_path}: {e}")
            return ""
        return "\n".join(text_parts)

    def _extract_text_file(self, file_path: Path) -> str:
        """æå–æ–‡æœ¬æ–‡ä»¶å†…å®¹"""
        with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
            return f.read()

    def _chunk_text(self, text: str, filename: str) -> None:
        """å°†æ–‡æœ¬åˆ†å—"""
        rag_config = self.config.rag
        chunk_size = rag_config.chunk_size
        overlap = rag_config.chunk_overlap
        min_length = rag_config.min_chunk_length

        for i in range(0, len(text), chunk_size - overlap):
            chunk = text[i:i + chunk_size]
            if len(chunk) > min_length:
                self.documents.append(chunk)
                self.filenames.append(filename)
                self.chunk_metadata.append({
                    "start": i,
                    "end": i + len(chunk),
                    "filename": filename
                })

    def query(self, user_query: str, top_k: Optional[int] = None) -> List[SearchResult]:
        """
        æ£€ç´¢ç›¸å…³æ–‡æ¡£

        Args:
            user_query: ç”¨æˆ·æŸ¥è¯¢
            top_k: è¿”å›ç»“æœæ•°é‡

        Returns:
            æ£€ç´¢ç»“æœåˆ—è¡¨ï¼ŒæŒ‰ç›¸å…³åº¦é™åº
        """
        if self.tfidf_matrix is None or not self.documents:
            return []

        top_k = top_k or self.config.rag.top_k

        # æ¸…æ´—æŸ¥è¯¢ï¼ˆç§»é™¤å¸¸è§åœç”¨è¯ï¼‰
        cleaned_query = self._clean_query(user_query)

        # å‘é‡åŒ–å¹¶è®¡ç®—ç›¸ä¼¼åº¦
        query_vec = self.vectorizer.transform([cleaned_query])
        similarities = cosine_similarity(query_vec, self.tfidf_matrix).flatten()

        # è·å– top-k ç»“æœ
        top_indices = similarities.argsort()[:-top_k-1:-1]

        results = []
        threshold = self.config.rag.similarity_threshold

        for idx in top_indices:
            if similarities[idx] > threshold:
                results.append(SearchResult(
                    source=self.filenames[idx],
                    content=self.documents[idx].strip(),
                    score=float(similarities[idx])
                ))

        return results

    def _clean_query(self, query: str) -> str:
        """æ¸…æ´—ç”¨æˆ·æŸ¥è¯¢"""
        # ç§»é™¤å¸¸è§ä¸­æ–‡åœç”¨è¯å’Œæ ‡ç‚¹
        stopwords = r'[ä»€ä¹ˆæ˜¯çš„ï¼Ÿ?å—å¦‚ä½•æ€ä¹ˆä¸ºä»€ä¹ˆ\s]'
        cleaned = re.sub(stopwords, '', query)
        return cleaned if cleaned else query

    def generate_response(self, user_query: str) -> str:
        """
        ç”Ÿæˆ AI å›å¤

        Args:
            user_query: ç”¨æˆ·é—®é¢˜

        Returns:
            æ ¼å¼åŒ–çš„ AI å›å¤
        """
        results = self.query(user_query)

        if not results:
            return self._generate_not_found_response(user_query)

        return self._format_response(user_query, results)

    def _generate_not_found_response(self, query: str) -> str:
        """ç”Ÿæˆæœªæ‰¾åˆ°ç»“æœçš„å›å¤"""
        return (
            f"ğŸ¤– **AI åˆ†æ**ï¼š\n\n"
            f"åœ¨ç°æœ‰è¯¾ç¨‹èµ„æ–™ä¸­æš‚æœªæ‰¾åˆ°å…³äºã€Œ{query}ã€çš„å…·ä½“æè¿°ã€‚\n\n"
            f"**å»ºè®®**ï¼š\n"
            f"- å°è¯•ä½¿ç”¨æ›´å…·ä½“çš„å…³é”®è¯\n"
            f"- æ£€æŸ¥ assets/corpus ç›®å½•ä¸‹æ˜¯å¦å·²æ·»åŠ ç›¸å…³æ–‡æ¡£"
        )

    def _format_response(self, query: str, results: List[SearchResult]) -> str:
        """æ ¼å¼åŒ–æ£€ç´¢ç»“æœä¸ºå›å¤"""
        response = f"ğŸ¤– **åŸºäºæ ¡å†…è¯¾ç¨‹èµ„æ–™çš„ AI å›å¤**ï¼š\n\n"
        response += f"å…³äºã€Œ**{query}**ã€ï¼Œæˆ‘åœ¨èµ„æ–™åº“ä¸­æ‰¾åˆ°äº†ç›¸å…³å†…å®¹ï¼š\n\n"

        for i, res in enumerate(results, 1):
            clean_content = res.content.replace('\n', ' ')
            response += f"> **ğŸ“‘ æ¥æºï¼š{res.source}** (åŒ¹é…åº¦: {res.score:.2f})\n"
            response += f"> *\"...{clean_content}...\"*\n\n"

        return response

    def get_stats(self) -> Dict:
        """è·å–å¼•æ“ç»Ÿè®¡ä¿¡æ¯"""
        return {
            "total_chunks": len(self.documents),
            "total_files": len(set(self.filenames)),
            "files": list(set(self.filenames)),
            "indexed": self.tfidf_matrix is not None
        }


# å…¨å±€ RAG å¼•æ“å®ä¾‹
_rag_instance: Optional[RAGEngine] = None

def get_rag_engine() -> RAGEngine:
    """è·å–å…¨å±€ RAG å¼•æ“å®ä¾‹"""
    global _rag_instance
    if _rag_instance is None:
        _rag_instance = RAGEngine()
    return _rag_instance
