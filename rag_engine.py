import os
import PyPDF2
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np
import re

class LocalRAG:
    def __init__(self, corpus_path):
        self.corpus_path = corpus_path
        self.documents = []  
        self.filenames = []  
        # ç»ˆæä¿®æ”¹ï¼šä½¿ç”¨å­—ç¬¦çº§ N-gram åŒ¹é… (2åˆ°4ä¸ªå­—ç¬¦)ï¼Œå®Œç¾é€‚é…ä¸­æ–‡
        self.vectorizer = TfidfVectorizer(analyzer='char_wb', ngram_range=(2, 4))
        self.tfidf_matrix = None
        self._load_and_index()

    def _load_and_index(self):
        if not os.path.exists(self.corpus_path):
            return

        for root, dirs, files in os.walk(self.corpus_path):
            for file in files:
                file_path = os.path.join(root, file)
                try:
                    text = ""
                    if file.lower().endswith('.pdf'):
                        with open(file_path, 'rb') as f:
                            reader = PyPDF2.PdfReader(f)
                            for page in reader.pages:
                                page_text = page.extract_text()
                                if page_text:
                                    text += page_text + "\n"
                    elif file.lower().endswith(('.txt', '.md')):
                        with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                            text = f.read()
                    
                    if text:
                        # å†æ¬¡ç¼©å°å—å¤§å°ï¼Œæé«˜åŒ¹é…ç²¾åº¦
                        chunk_size = 200
                        overlap = 40
                        for i in range(0, len(text), chunk_size - overlap):
                            chunk = text[i:i + chunk_size]
                            if len(chunk) > 10: 
                                self.documents.append(chunk)
                                self.filenames.append(file)
                                
                except Exception as e:
                    print(f"Error reading {file}: {e}")

        if self.documents:
            self.tfidf_matrix = self.vectorizer.fit_transform(self.documents)

    def query(self, user_query, top_k=2):
        if self.tfidf_matrix is None or not self.documents:
            return []

        # ç®€å•æ¸…æ´—æé—®
        q = re.sub(r'[ä»€ä¹ˆæ˜¯çš„ï¼Ÿ?å—å¦‚ä½•\s]', '', user_query)
        if not q: q = user_query

        query_vec = self.vectorizer.transform([q])
        cosine_similarities = cosine_similarity(query_vec, self.tfidf_matrix).flatten()
        
        related_docs_indices = cosine_similarities.argsort()[:-top_k-1:-1]
        
        results = []
        for idx in related_docs_indices:
            # åªè¦æœ‰ä¸€å®šç›¸å…³åº¦å°±è¿”å›
            if cosine_similarities[idx] > 0.02: 
                results.append({
                    "source": self.filenames[idx],
                    "content": self.documents[idx].strip(),
                    "score": cosine_similarities[idx]
                })
        
        return results

    def generate_response(self, user_query):
        results = self.query(user_query)
        
        if not results:
            return f"ğŸ¤– **AI åˆ†æ**ï¼šåœ¨ç°æœ‰è¯¾ç¨‹èµ„æ–™ä¸­æš‚æœªæ‰¾åˆ°å…³äºâ€œ{user_query}â€çš„å…·ä½“æè¿°ã€‚å»ºè®®æ£€æŸ¥ CV/assets/corpus ç›®å½•ä¸‹çš„æ–‡æ¡£å†…å®¹ã€‚"

        response = f"ğŸ¤– **åŸºäºæ ¡å†…è¯¾ç¨‹èµ„æ–™çš„ AI å›å¤**ï¼š\n\n"
        response += f"å…³äºâ€œ**{user_query}**â€ï¼Œæˆ‘åœ¨èµ„æ–™åº“ä¸­æ‰¾åˆ°äº†ç›¸å…³çº¿ç´¢ï¼š\n\n"
        
        for i, res in enumerate(results, 1):
            clean_content = res['content'].replace('\n', ' ')
            response += f"> **ğŸ“‘ æ¥æºï¼š{res['source']}** (åŒ¹é…åº¦: {res['score']:.2f})\n"
            response += f"> *â€œ...{clean_content}...â€*\n\n"
            
        return response

_rag_instance = None
def get_rag_engine():
    global _rag_instance
    if _rag_instance is None:
        _rag_instance = LocalRAG('CV/assets/corpus')
    return _rag_instance
