"""
AI æ•™å­¦åŠ©æ•™é¡µé¢

æä¾›åŸºäº RAG çš„æ™ºèƒ½é—®ç­”åŠŸèƒ½ã€‚
"""
from __future__ import annotations

import streamlit as st

from core.rag_engine import get_rag_engine, RAGEngine


@st.cache_resource(show_spinner="æ­£åœ¨åŠ è½½çŸ¥è¯†åº“...")
def _get_cached_rag_engine() -> RAGEngine:
    """è·å–ç¼“å­˜çš„ RAG å¼•æ“å®ä¾‹

    ä½¿ç”¨ Streamlit çš„ cache_resource è£…é¥°å™¨ç¡®ä¿
    RAG å¼•æ“åªåˆå§‹åŒ–ä¸€æ¬¡ï¼Œé¿å…é‡å¤åŠ è½½è¯­æ–™åº“ã€‚
    """
    return get_rag_engine()


def render_ai_copilot():
    """æ¸²æŸ“ AI æ•™å­¦åŠ©æ•™é¡µé¢"""
    st.header("ğŸ¤– AI æ•™å­¦åŠ©æ•™ (Teaching Copilot)")
    st.caption("å†…ç½®ã€Šæ–°é—»å­¦æ¦‚è®ºã€‹ã€ã€Šä¼ æ’­å­¦æ•™ç¨‹ã€‹åŠå¤§å‚ JD çŸ¥è¯†åº“")

    col_chat, col_context = st.columns([2, 1])

    # ==================== å³æ : å­¦ä¹ èµ„æº ====================
    with col_context:
        _render_learning_resources()

    # ==================== å·¦æ : èŠå¤©ç•Œé¢ ====================
    with col_chat:
        _render_chat_interface()


def _render_learning_resources():
    """æ¸²æŸ“å­¦ä¹ èµ„æºæ¨è"""
    st.subheader("ğŸ“š æ¨èå­¦ä¹ èµ„æº")

    st.markdown("""
    æ ¹æ®ä½ çš„ç®€å†çŸ­æ¿ï¼Œä¸ºä½ æ¨èï¼š

    **ğŸ“– é˜…è¯»**
    - ã€Šç²¾ç¡®æ–°é—»æŠ¥é“ã€‹ (Philip Meyer) - *é’ˆå¯¹æ•°æ®åˆ†æèƒ½åŠ›*
    - æ™®åˆ©ç­–å¥–éè™šæ„å†™ä½œç‰¹ç¨¿é€‰ - *é’ˆå¯¹å™äº‹èƒ½åŠ›*

    **â–¶ï¸ è¯¾ç¨‹**
    - [Coursera] Python for Data Science
    - [Bilibili] è´¢æ–°è§†é¢‘éƒ¨ï¼šå¦‚ä½•åšçŸ­è§†é¢‘å™äº‹
    """)

    st.info(
        "ğŸ’¡ **æé—®æç¤º**ï¼š\n"
        "- 'å¦‚ä½•ç”¨ STAR æ³•åˆ™ä¼˜åŒ–æˆ‘çš„å®ä¹ ç»å†ï¼Ÿ'\n"
        "- 'ä»€ä¹ˆæ˜¯å€’é‡‘å­—å¡”ç»“æ„ï¼Ÿ'\n"
        "- 'åšæ•°æ®æ–°é—»éœ€è¦å­¦ä»€ä¹ˆï¼Ÿ'"
    )

    # æ˜¾ç¤º RAG å¼•æ“çŠ¶æ€
    _render_rag_status()


def _render_rag_status():
    """æ¸²æŸ“ RAG å¼•æ“çŠ¶æ€"""
    with st.expander("ğŸ”§ çŸ¥è¯†åº“çŠ¶æ€"):
        try:
            engine = _get_cached_rag_engine()
            stats = engine.get_stats()

            if stats['indexed']:
                st.success(f"âœ… å·²ç´¢å¼• {stats['total_chunks']} ä¸ªæ–‡æœ¬å—")
                st.write(f"æ¥æºæ–‡ä»¶: {stats['total_files']} ä¸ª")
                for f in stats['files']:
                    st.caption(f"  - {f}")
            else:
                st.warning("âš ï¸ çŸ¥è¯†åº“å°šæœªç´¢å¼•")
        except Exception as e:
            st.error(f"å¼•æ“çŠ¶æ€è·å–å¤±è´¥: {e}")


def _render_chat_interface():
    """æ¸²æŸ“èŠå¤©ç•Œé¢"""
    # åˆå§‹åŒ–èŠå¤©å†å²
    if 'chat_history' not in st.session_state:
        st.session_state.chat_history = [
            {
                "role": "assistant",
                "content": "ä½ å¥½ï¼æˆ‘æ˜¯ä½ çš„ AI æ•™å­¦åŠ©æ‰‹ã€‚å…³äºæ–°é—»é‡‡å†™ã€ç®€å†ä¼˜åŒ–æˆ–è¯¾ç¨‹çŸ¥è¯†ç‚¹ï¼Œéšæ—¶é—®æˆ‘ã€‚"
            }
        ]

    # æ˜¾ç¤ºå†å²æ¶ˆæ¯
    for message in st.session_state.chat_history:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    # è¾“å…¥æ¡†
    if prompt := st.chat_input("å‘ AI åŠ©æ•™æé—®..."):
        _handle_user_input(prompt)


def _handle_user_input(prompt: str):
    """å¤„ç†ç”¨æˆ·è¾“å…¥"""
    # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯
    st.session_state.chat_history.append({
        "role": "user",
        "content": prompt
    })

    with st.chat_message("user"):
        st.markdown(prompt)

    # ç”Ÿæˆ AI å›å¤
    with st.chat_message("assistant"):
        with st.spinner("æ­£åœ¨æ£€ç´¢è¯¾ç¨‹èµ„æ–™..."):
            response_text = _query_knowledge_base(prompt)
        st.markdown(response_text)

    st.session_state.chat_history.append({
        "role": "assistant",
        "content": response_text
    })


def _query_knowledge_base(query: str) -> str:
    """æŸ¥è¯¢çŸ¥è¯†åº“

    Args:
        query: ç”¨æˆ·æŸ¥è¯¢

    Returns:
        AI ç”Ÿæˆçš„å›å¤
    """
    try:
        engine = _get_cached_rag_engine()
        return engine.generate_response(query)
    except Exception as e:
        return f"âš ï¸ RAG å¼•æ“è¿è¡Œå‡ºé”™: {str(e)}"
